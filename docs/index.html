<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=1,user-scalable=no"><title>ADA 2018 - Exploring customer behavior on Amazon</title><link rel="icon" href="assets/images/ada-logo.png" type="image/png"><link href="https://fonts.googleapis.com/css?family=Indie+Flower" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Zilla+Slab" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Sacramento" rel="stylesheet"><link rel="stylesheet" href="styles.css?35"><script src="https://use.fontawesome.com/0cebcc1cf0.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-xxxxxx-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-xxxxxx-1');</script></head><body><div class="static-header-hidden" id="static-header"><img src="assets/images/ada-logo.png"><h1>ADA 2018</h1></div><div id="page-content"><div class="page-bloc" id="page-header"><h1 id="page-title">Exploring customer behavior on Amazon</h1><p>Started 20 years ago, Amazon completely reshaped the online market. Amazon has more than half a billion different products within very diversified categories (book, kitchen equipment, jewelry..). This growth had an impact on how people sell and buy on the platform. Because there are so many products, we might think that customer reviews have a crucial importance on the choices customer make before buying a product.</p><p>Because customer reviews are a form of social interaction within the online marketplace, we aim to find out in what way customer reviews are relevant, and what is their overall influence on the product success. Is a first bad review a red flag for customers?</p><p>In the first steps of our analysis, we explored many aspects of our data, and we thought it is a good idea to work on every category of products separately, since our analysis is mainly textual, every category has its own lexical aspects to conserve, and then compare the overall behavior.</p><br><p class="links"><a href="https://github.com/aminetourari" target="_blank">Mohammed Amine TOURARI</a><a href="https://benjamin-raymond.pro" target="_blank">Benjamin RAYMOND</a><a href="https://github.com/youssefkit" target="_blank">Youssef KITANE</a></p><p class="links"><a href="https://github.com/epfl-ada-project-amazon/data-analysis" target="_blank">See on github</a></p></div><hr class="page-separator"><div id="page-body"><div id="page-tab-controls"><div class="tab-select"><input class="tab-select-radio" id="cat-1" type="radio" name="tab-select" value="0" checked="checked"><label for="cat-1">Pets supplies</label></div><div class="tab-select"><input class="tab-select-radio" id="cat-2" type="radio" name="tab-select" value="1"><label for="cat-2">Home & kitchen</label></div><div class="tab-select"><input class="tab-select-radio" id="cat-3" type="radio" name="tab-select" value="2"><label for="cat-3">Video games</label></div></div><div id="page-tabs"><div class="page-tab tab-hidden" id="tab-0"><div class="item-0 item item-template0"><div class="item-element item-image"><img src="assets/images/pets/01.png"></div><div class="item-element item-text">Most of the ratings are good (5’s and 4’s), which mean that customer’s should be pretty satisfied. We’re going to go and look closer the the textual reviews to see if they confirm this hypothesis (hopefully otherwise we’re quite in trouble)</div></div><div class="item-1 item item-template0"><div class="item-element item-text">This plot is special, we can see that people who review a lot of products are not numerous, and then the majority of reviews are from peple who have less then 50 reviews. The shape is unique and we kan see that many people give the same ratings at each time ( not so likely that the mean is an integer), we can see that on horizontal lines of the (1,2,3,4 and 5 stars ratings).</div><div class="item-element item-image"><img src="assets/images/pets/02.png"></div></div><div class="item-2 item item-template3"><div class="item-element item-text">We have used VaderSentiment analyser to get the sentiment classification based on the compound. Even if this analyser was trained on tweets, we assume in a first place that the content is going to be almost the same. BUT, we don’t only have textual reviews but also a summary of the reviews that should basically summarize the summary. Hence, the sentiment class should be the same as the main textual reviews, we’re going to apply the sentiment analyser on both features and see how does it stick to the numerical ratings.</div></div><div class="item-3 item item-template1"><div class="item-element item-image"><img src="assets/images/pets/03.png"></div><div class="item-element item-text">No problem seen with the classified positive reviews, since the majority of them have 5’s and 4’s reviews. We don’t need to dig deeper for the classified neutral reviews. However, it’s interesting to focus on the classified negative reviews since we have a lot of 5’s in there. Something strange is going on !! Let’s check the distribution of the reviews’s based on the summary classification of sentiment</div></div><div class="item-4 item item-template1"><div class="item-element item-image"><img src="assets/images/pets/04.png"></div><div class="item-element item-text">We can observe the same thing for positive and neutral, but we have less 5’s for the classified negative reviews. The sentiment analysis was made by the same analyser, which confirm that there is a problem somewhere. We’re getting closer !</div></div><div class="item-5 item item-template3"><div class="item-element item-text">Now we’re going the show the proportions of summary classification on the 3 categories of classified reviews (positive, negative and neutral) We’re not going to handle neutral reviews because it’s always hard and try to look for insights in doubtful data (neutral can be anything that’s not extreme)</div></div><div class="item-6 item item-template1"><div class="item-element item-image"><img src="assets/images/pets/05.png"></div><div class="item-element item-text">For positive reviews, we have a minority of negative summaries. It’s strange but not alarming. The alarming case is that reviews with negative sentiment on review and summary are less then 30 %, which somehow confirmes that we do have a real problem here !</div></div><div class="item-7 item item-template2"><div class="item-element item-text">We chose to display the dominant words in both summaries (left) and reviews (right) separately, for the reviews who had positive summary and negative review content ! The size of each word depends on the number of occurrences.</div><div class="item-element item-image"><img src="assets/images/pets/06.png"><img src="assets/images/pets/07.png"></div></div><div class="item-8 item item-template3"><div class="item-element item-text">The presence of words like : « Great, good, ok, fun, ,,, » in summaries of negative reviews is quite strange and incoherent. It’s normal to get a positive prediction. It’s also misleading because the classifier is much more likely to make mistakes on long texts then on a short summary.</div></div><div class="item-9 item item-template0"><div class="item-element item-text">We are going to build and train our own model in order to try and predict the sentiments from the text review.<br>To do that, we used a Long Short Term Memory network, which is a special kind of RNN, capable of learning long-term dependencies. They work tremendously well on a large variety of problems of natural language processing, and are now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! It is very practical in our case because it aims to detect tone's changement in a review to maximize the chances of a successful learning. We train this model with 70% of the reviews. To do so, we defined a threshold in numerical ratings (the threshold that gave us the best accuracy) which is to consider:<ul><li>5's and 4's as positive</li><li>3's and 2's and ones as negative!</li></ul>With this configuration, we have the following results presented in the confusion matrix<br><br>Accuracy : <b>91.8</b> %<br>P.S : the negative and positive are inverted</div><div class="item-element item-image"><img src="assets/images/pets/08.png"></div></div><div class="item-10 item item-template3"><div class="item-element item-text"><h2>Closure</h2></div></div><div class="item-11 item item-template3"><div class="item-element item-text"><br>This analysis was quite long and tiring, with many moments of hesitation and ideas' dropping. But, we finally focused on deep textual analysis and differences between the summary and the main review. We know now for sure that the reviews (and every text written by customers on people on social media) is not 100% reliable. In fact, even if everything was fine and coherent, NLP is very subjective and everything depends on small details: (choice of words, text's lenght, capitalization..)<br><br>After training our own model, we achieved good predictions and overall scores, but it is always hard to deal with what people write. Sometimes they are happy with the products but still complain about a detail, or they use a neutral language which makes it even harder to detect the sentiment. Some customers are funny or sarcastic, which is sometimes difficult for humans to understand, let alone an algorithm! (the famous summary "TOO GOOD TO BE TRUE").<br>We worked on different categories to try and see different behavior, which is kind of true when we focus on the lexical background of users but the overall insights are present everywhere! Such datasets and very rich and can serve for many other topics, but it is also difficult to work with without making assumptions, or at least without checking the veracity of each claim we made.</div></div></div><div class="page-tab tab-hidden" id="tab-1"><div class="item-0 item item-template0"><div class="item-element item-image"><img src="assets/images/kitchen/01.png"></div><div class="item-element item-text">Most of the ratings are good (5’s and 4’s), which mean that customer’s should be pretty satisfied. We’re going to go and look closer the the textual reviews to see if they confirm this hypothesis (hopefully otherwise we’re quite in trouble)</div></div><div class="item-1 item item-template0"><div class="item-element item-text">Same as "Pet supplies", a spcial distribution but in addition, we can see that customers with more than 100 reviews have a high mean rating ( > 3 ). It means that only customers satisfied with the products keep rating Amazon's items, the others either stop reviewing, or stop buying!</div><div class="item-element item-image"><img src="assets/images/kitchen/02.png"></div></div><div class="item-2 item item-template3"><div class="item-element item-text">We have used VaderSentiment analyser to get the sentiment classification based on the compound. Even if this analyser was trained on tweets, we assume in a first place that the content is going to be almost the same. BUT, we don’t only have textual reviews but also a summary of the reviews that should basically summarize the summary. Hence, the sentiment class should be the same as the main textual reviews, we’re going to apply the sentiment analyser on both features and see how does it stick to the numerical ratings.</div></div><div class="item-3 item item-template1"><div class="item-element item-image"><img src="assets/images/kitchen/03.png"></div><div class="item-element item-text">No problem seen with the classified positive reviews, since the majority of them have 5’s and 4’s reviews. We don’t need to dig deeper for the classified neutral reviews. However, it’s interesting to focus on the classified negative reviews since we have a lot of 5’s in there. Something strange is going on !! Let’s check the distribution of the reviews’s based on the summary classification of sentiment</div></div><div class="item-4 item item-template1"><div class="item-element item-image"><img src="assets/images/kitchen/04.png"></div><div class="item-element item-text">We can observe the same thing for positive and neutral, but we have less 5’s for the classified negative reviews. The sentiment analysis was made by the same analyser, which confirm that there is a problem somewhere. We’re getting closer !</div></div><div class="item-5 item item-template3"><div class="item-element item-text">Now we’re going the show the proportions of summary classification on the 3 categories of classified reviews (positive, negative and neutral) We’re not going to handle neutral reviews because it’s always hard and try to look for insights in doubtful data (neutral can be anything that’s not extreme)</div></div><div class="item-6 item item-template1"><div class="item-element item-image"><img src="assets/images/kitchen/05.png"></div><div class="item-element item-text">For positive reviews, we have a minority of negative summaries. It’s strange but not alarming. The alarming case is that reviews with negative sentiment on review and summary are less then 35 %, which somehow confirmes that we do have a real problem here !</div></div><div class="item-7 item item-template2"><div class="item-element item-text">We chose to display the dominant words in both summaries (left) and reviews (right) separately, for the reviews who had positive summary and negative review content ! The size of each word depends on the number of occurrences.</div><div class="item-element item-image"><img src="assets/images/kitchen/06.png"><img src="assets/images/kitchen/07.png"></div></div><div class="item-8 item item-template3"><div class="item-element item-text">The presence of words like : « Nice, perfect, excellent, works, well, best loved, love, awesome ... » in summaries of negative reviews is quite strange and incoherent. It’s normal to get a positive prediction. It’s also misleading because the classifier is much more likely to make mistakes on long texts then on a short summary.</div></div><div class="item-9 item item-template0"><div class="item-element item-text">Same model, same threshold as for the pets supplies analysis<br><br>Accuracy : <b>91.26</b> %<br>P.S : the negative and positive are inverted</div><div class="item-element item-image"><img src="assets/images/kitchen/08.png"></div></div><div class="item-10 item item-template3"><div class="item-element item-text"><h2>Closure</h2></div></div><div class="item-11 item item-template3"><div class="item-element item-text"><br>This analysis was quite long and tiring, with many moments of hesitation and ideas' dropping. But, we finally focused on deep textual analysis and differences between the summary and the main review. We know now for sure that the reviews (and every text written by customers on people on social media) is not 100% reliable. In fact, even if everything was fine and coherent, NLP is very subjective and everything depends on small details: (choice of words, text's lenght, capitalization..)<br><br>After training our own model, we achieved good predictions and overall scores, but it is always hard to deal with what people write. Sometimes they are happy with the products but still complain about a detail, or they use a neutral language which makes it even harder to detect the sentiment. Some customers are funny or sarcastic, which is sometimes difficult for humans to understand, let alone an algorithm! (the famous summary "TOO GOOD TO BE TRUE").<br>We worked on different categories to try and see different behavior, which is kind of true when we focus on the lexical background of users but the overall insights are present everywhere! Such datasets and very rich and can serve for many other topics, but it is also difficult to work with without making assumptions, or at least without checking the veracity of each claim we made.</div></div></div><div class="page-tab tab-hidden" id="tab-2"><div class="item-0 item item-template0"><div class="item-element item-image"><img src="assets/images/video-games/01.png"></div><div class="item-element item-text">Most of the ratings are good (5’s and 4’s), which mean that customer’s should be pretty satisfied. We’re going to go and look closer the the textual reviews to see if they confirm this hypothesis (hopefully otherwise we’re quite in trouble)</div></div><div class="item-1 item item-template0"><div class="item-element item-text">This plot is special, we can see that people who review a lot of products are not numerous, and then the majority of reviews are from peple who have less then 50 reviews. The shape is unique and we kan see that many people give the same ratings at each time ( not so likely that the mean is an integer), we can see that on horizontal lines of the (1,2,3,4 and 5 stars ratings).</div><div class="item-element item-image"><img src="assets/images/video-games/02.png"></div></div><div class="item-2 item item-template3"><div class="item-element item-text">We have used VaderSentiment analyser to get the sentiment classification based on the compound. Even if this analyser was trained on tweets, we assume in a first place that the content is going to be almost the same. BUT, we don’t only have textual reviews but also a summary of the reviews that should basically summarize the summary. Hence, the sentiment class should be the same as the main textual reviews, we’re going to apply the sentiment analyser on both features and see how does it stick to the numerical ratings.</div></div><div class="item-3 item item-template1"><div class="item-element item-image"><img src="assets/images/video-games/03.png"></div><div class="item-element item-text">No problem seen with the classified positive reviews, since the majority of them have 5’s and 4’s reviews. We don’t need to dig deeper for the classified neutral reviews. However, it’s interesting to focus on the classified negative reviews since we have a lot of 5’s in there. Something strange is going on !! Let’s check the distribution of the reviews’s based on the summary classification of sentiment</div></div><div class="item-4 item item-template1"><div class="item-element item-image"><img src="assets/images/video-games/04.png"></div><div class="item-element item-text">We can observe the same thing for positive and neutral, but we have less 5’s for the classified negative reviews. The sentiment analysis was made by the same analyser, which confirm that there is a problem somewhere. We’re getting closer !</div></div><div class="item-5 item item-template3"><div class="item-element item-text">Now we’re going the show the proportions of summary classification on the 3 categories of classified reviews (positive, negative and neutral) We’re not going to handle neutral reviews because it’s always hard and try to look for insights in doubtful data (neutral can be anything that’s not extreme)</div></div><div class="item-6 item item-template1"><div class="item-element item-image"><img src="assets/images/video-games/05.png"></div><div class="item-element item-text">For positive reviews, we have a minority of negative summaries. It’s strange but not alarming. The alarming case is that reviews with negative sentiment on review and summary are less then 35 %, which somehow confirmes that we do have a real problem here !</div></div><div class="item-7 item item-template2"><div class="item-element item-text">We chose to display the dominant words in both summaries (left) and reviews (right) separately, for the reviews who had positive summary and negative review content ! The size of each word depends on the number of occurrences.</div><div class="item-element item-image"><img src="assets/images/video-games/06.png"><img src="assets/images/video-games/07.png"></div></div><div class="item-8 item item-template3"><div class="item-element item-text">The presence of words like : « Great, better, good, awesome, nice, ,,, » in summaries of negative reviews is quite strange and incoherent. It’s normal to get a positive prediction. It’s also misleading because the classifier is much more likely to make mistakes on long texts then on a short summary.</div></div><div class="item-9 item item-template0"><div class="item-element item-text">Same model, same threshold as for the pets supplies analysis<br><br>Accuracy : <b>95.94</b> % (best accuracy so far but we had less reviews than other categories)<br>P.S : the negative and positive are inverted</div><div class="item-element item-image"><img src="assets/images/video-games/08.png"></div></div><div class="item-10 item item-template3"><div class="item-element item-text"><h2>Closure</h2></div></div><div class="item-11 item item-template3"><div class="item-element item-text"><br>This analysis was quite long and tiring, with many moments of hesitation and ideas' dropping. But, we finally focused on deep textual analysis and differences between the summary and the main review. We know now for sure that the reviews (and every text written by customers on people on social media) is not 100% reliable. In fact, even if everything was fine and coherent, NLP is very subjective and everything depends on small details: (choice of words, text's lenght, capitalization..)<br><br>After training our own model, we achieved good predictions and overall scores, but it is always hard to deal with what people write. Sometimes they are happy with the products but still complain about a detail, or they use a neutral language which makes it even harder to detect the sentiment. Some customers are funny or sarcastic, which is sometimes difficult for humans to understand, let alone an algorithm! (the famous summary "TOO GOOD TO BE TRUE").<br>We worked on different categories to try and see different behavior, which is kind of true when we focus on the lexical background of users but the overall insights are present everywhere! Such datasets and very rich and can serve for many other topics, but it is also difficult to work with without making assumptions, or at least without checking the veracity of each claim we made.</div></div></div></div></div></div></body><script src="bundle.js?35"></script></html>